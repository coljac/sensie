
@article{montavonMethodsInterpretingUnderstanding2018,
  title = {Methods for Interpreting and Understanding Deep Neural Networks},
  author = {Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  year = {2018},
  month = feb,
  volume = {73},
  pages = {1--15},
  issn = {1051-2004},
  doi = {10.1016/j.dsp.2017.10.011},
  url = {http://www.sciencedirect.com/science/article/pii/S1051200417302385},
  urldate = {2019-04-16},
  abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. As a tutorial paper, the set of methods covered here is not exhaustive, but sufficiently representative to discuss a number of questions in interpretability, technical challenges, and possible applications. The second part of the tutorial focuses on the recently proposed layer-wise relevance propagation (LRP) technique, for which we provide theory, recommendations, and tricks, to make most efficient use of it on real data.},
  file = {/home/coljac/Zotero/storage/DDW4XEHI/DDW4XEHI.pdf;/home/coljac/Zotero/storage/8DNL44EE/S1051200417302385.html},
  journal = {Digital Signal Processing},
  keywords = {Activation maximization,Deep neural networks,Layer-wise relevance propagation,Sensitivity analysis,Taylor decomposition}
}




@inproceedings{bachControllingExplanatoryHeatmap2016,
  title = {Controlling Explanatory Heatmap Resolution and Semantics via Decomposition Depth},
  booktitle = {2016 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Bach, Sebastian and Binder, Alexander and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  year = {2016},
  month = sep,
  pages = {2271--2275},
  issn = {2381-8549},
  doi = {10.1109/ICIP.2016.7532763},
  abstract = {We present an application of the Layer-wise Relevance Propagation (LRP) algorithm to state of the art deep convolutional neural networks and Fisher Vector classifiers to compare the image perception and prediction strategies of both classifiers with the use of visualized heatmaps. Layer-wise Relevance Propagation (LRP) is a method to compute scores for individual components of an input image, denoting their contribution to the prediction of the classifier for one particular test point. We demonstrate the impact of different choices of decomposition cut-off points during the LRP-process, controlling the resolution and semantics of the heatmap on test images from the PASCAL VOC 2007 test data set.},
  file = {/home/coljac/Zotero/storage/E5RXQWYL/Bach et al. - 2016 - Controlling explanatory heatmap resolution and sem.pdf;/home/coljac/Zotero/storage/C3I3V7ZP/7532763.html},
  keywords = {Classification algorithms,Computational modeling,Computer architecture,convolution,Convolution,decomposition depth,deep convolutional neural networks,Explaining Classifiers,explanatory heatmap resolution,explanatory heatmap semantics,Fisher vector classifiers,Heatmapping,image classification,image perception,image prediction,image resolution,layer-wise relevance propagation,LRP algorithm,Mathematical model,neural nets,Neural networks,PASCAL VOC 2007 test data set,Semantics,System analysis and design,vectors,visualized heatmaps}
}

@article{bachPixelWiseExplanationsNonLinear2015,
  title = {On {{Pixel}}-{{Wise Explanations}} for {{Non}}-{{Linear Classifier Decisions}} by {{Layer}}-{{Wise Relevance Propagation}}},
  author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  year = {2015},
  month = jul,
  volume = {10},
  pages = {e0130140},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0130140},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140},
  urldate = {2019-07-13},
  abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
  file = {/home/coljac/Zotero/storage/4XCK7F9Y/Bach et al. - 2015 - On Pixel-Wise Explanations for Non-Linear Classifi.pdf;/home/coljac/Zotero/storage/KQSC3T6S/article.html},
  journal = {PLOS ONE},
  keywords = {Algorithms,Coding mechanisms,Imaging techniques,Kernel functions,Kernel methods,Neural networks,Neurons,Vision},
  language = {en},
  number = {7}
}

@inproceedings{binderLayerwiseRelevancePropagation2016,
  title = {Layer-Wise Relevance Propagation for Neural Networks with Local Renormalization Layers},
  booktitle = {International {{Conference}} on {{Artificial Neural Networks}}},
  author = {Binder, Alexander and Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  year = {2016},
  pages = {63--71},
  publisher = {{Springer}},
  file = {/home/coljac/Zotero/storage/7TLNISSD/Binder et al. - 2016 - Layer-wise relevance propagation for neural networ.pdf;/home/coljac/Zotero/storage/TRFVPTUW/layerwise.pdf;/home/coljac/Zotero/storage/IWEL5W8K/978-3-319-44781-0_8.html}
}

@article{flukeSurveyingReachMaturity2020,
  title = {Surveying the Reach and Maturity of Machine Learning and Artificial Intelligence in Astronomy},
  author = {Fluke, Christopher J. and Jacobs, Colin},
  year = {2020},
  volume = {10},
  pages = {e1349},
  issn = {1942-4795},
  doi = {10.1002/widm.1349},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1349},
  urldate = {2020-02-16},
  abstract = {Machine learning (automated processes that learn by example in order to classify, predict, discover, or generate new data) and artificial intelligence (methods by which a computer makes decisions or discoveries that would usually require human intelligence) are now firmly established in astronomy. Every week, new applications of machine learning and artificial intelligence are added to a growing corpus of work. Random forests, support vector machines, and neural networks are now having a genuine impact for applications as diverse as discovering extrasolar planets, transient objects, quasars, and gravitationally lensed systems, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. This review surveys contemporary, published literature on machine learning and artificial intelligence in astronomy and astrophysics. Applications span seven main categories of activity: classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights. These categories form the basis of a hierarchy of maturity, as the use of machine learning and artificial intelligence emerges, progresses, or becomes established. This article is categorized under: Application Areas {$>$} Science and Technology Fundamental Concepts of Data and Knowledge {$>$} Motivation and Emergence of Data Mining Technologies {$>$} Machine Learning},
  copyright = {\textcopyright{} 2019 Wiley Periodicals, Inc.},
  file = {/home/coljac/Zotero/storage/DHG4IR5S/Fluke and Jacobs - 2020 - Surveying the reach and maturity of machine learni.pdf;/home/coljac/Zotero/storage/JJJD5I8L/widm.html},
  journal = {WIREs Data Mining and Knowledge Discovery},
  keywords = {artificial intelligence,astronomy,astrophysics,machine learning},
  language = {en},
  number = {2}
}

@article{jacobsExtendedCatalogGalaxy2019,
  title = {An {{Extended Catalog}} of {{Galaxy}}\textendash{{Galaxy Strong Gravitational Lenses Discovered}} in {{DES Using Convolutional Neural Networks}}},
  author = {Jacobs, C. and Collett, T. and Glazebrook, K. and {Buckley-Geer}, E. and Diehl, H. T. and Lin, H. and McCarthy, C. and Qin, A. K. and Odden, C. and Escudero, M. Caso and Dial, P. and Yung, V. J. and Gaitsch, S. and Pellico, A. and Lindgren, K. A. and Abbott, T. M. C. and Annis, J. and Avila, S. and Brooks, D. and Burke, D. L. and Rosell, A. Carnero and Kind, M. Carrasco and Carretero, J. and da Costa, L. N. and Vicente, J. De and Fosalba, P. and Frieman, J. and {Garc{\'i}a-Bellido}, J. and Gaztanaga, E. and Goldstein, D. A. and Gruen, D. and Gruendl, R. A. and Gschwend, J. and Hollowood, D. L. and Honscheid, K. and Hoyle, B. and James, D. J. and Krause, E. and Kuropatkin, N. and Lahav, O. and Lima, M. and Maia, M. A. G. and Marshall, J. L. and Miquel, R. and Plazas, A. A. and Roodman, A. and Sanchez, E. and Scarpine, V. and Serrano, S. and {Sevilla-Noarbe}, I. and Smith, M. and Sobreira, F. and Suchyta, E. and Swanson, M. E. C. and Tarle, G. and Vikram, V. and Walker, A. R. and {and}, Y. Zhang},
  year = {2019},
  month = jul,
  volume = {243},
  pages = {17},
  issn = {0067-0049},
  doi = {10.3847/1538-4365/ab26b6},
  url = {https://doi.org/10.3847\%2F1538-4365\%2Fab26b6},
  urldate = {2019-07-21},
  abstract = {We search Dark Energy Survey (DES) Year 3 imaging for galaxy\textendash{}galaxy strong gravitational lenses using convolutional neural networks, extending previous work with new training sets and covering a wider range of redshifts and colors. We train two neural networks using images of simulated lenses, then use them to score postage-stamp images of 7.9 million sources from DES chosen to have plausible lens colors based on simulations. We examine 1175 of the highest-scored candidates and identify 152 probable or definite lenses. Examining an additional 20,000 images with lower scores, we identify a further 247 probable or definite candidates. After including 86 candidates discovered in earlier searches using neural networks and 26 candidates discovered through visual inspection of blue-near-red objects in the DES catalog, we present a catalog of 511 lens candidates.},
  copyright = {All rights reserved},
  file = {/home/coljac/Zotero/storage/H4TEGPR6/Jacobs et al. - 2019 - An Extended Catalog of Galaxyâ€“Galaxy Strong Gravit.pdf},
  journal = {ApJS},
  language = {en},
  number = {1}
}

@article{jacobsFindingHighredshiftStrong2019,
  title = {Finding High-Redshift Strong Lenses in {{DES}} Using Convolutional Neural Networks},
  author = {Jacobs, C. and Collett, T. and Glazebrook, K. and McCarthy, C. and Qin, A. K. and Abbott, T. M. C. and Abdalla, F. B. and Annis, J. and Avila, S. and Bechtol, K. and Bertin, E. and Brooks, D. and {Buckley-Geer}, E. and Burke, D. L. and Carnero Rosell, A. and Carrasco Kind, M. and Carretero, J. and {da Costa}, L. N. and Davis, C. and De Vicente, J. and Desai, S. and Diehl, H. T. and Doel, P. and Eifler, T. F. and Flaugher, B. and Frieman, J. and {Garc{\'i}a-Bellido}, J. and Gaztanaga, E. and Gerdes, D. W. and Goldstein, D. A. and Gruen, D. and Gruendl, R. A. and Gschwend, J. and Gutierrez, G. and Hartley, W. G. and Hollowood, D. L. and Honscheid, K. and Hoyle, B. and James, D. J. and Kuehn, K. and Kuropatkin, N. and Lahav, O. and Li, T. S. and Lima, M. and Lin, H. and Maia, M. a. G. and Martini, P. and Miller, C. J. and Miquel, R. and Nord, B. and Plazas, A. A. and Sanchez, E. and Scarpine, V. and Schubnell, M. and Serrano, S. and {Sevilla-Noarbe}, I. and Smith, M. and {Soares-Santos}, M. and Sobreira, F. and Suchyta, E. and Swanson, M. E. C. and Tarle, G. and Vikram, V. and Walker, A. R. and Zhang, Y. and Zuntz, J.},
  year = {2019},
  month = apr,
  volume = {484},
  pages = {5330--5349},
  issn = {0035-8711},
  doi = {10.1093/mnras/stz272},
  url = {https://academic.oup.com/mnras/article/484/4/5330/5301418},
  urldate = {2019-02-21},
  abstract = {Abstract.  We search Dark Energy Survey (DES) Year 3 imaging data for galaxy\textendash{}galaxy strong gravitational lenses using convolutional neural networks. We generate},
  copyright = {All rights reserved},
  file = {/home/coljac/Zotero/storage/3QGT6A6I/Jacobs et al. - 2019 - Finding high-redshift strong lenses in DES using c.pdf;/home/coljac/Zotero/storage/22IVAMGA/5301418.html},
  journal = {Mon Not R Astron Soc},
  language = {en},
  number = {4}
}

@article{jacobsFindingStrongLenses2017,
  title = {Finding Strong Lenses in {{CFHTLS}} Using Convolutional Neural Networks},
  author = {Jacobs, C. and Glazebrook, K. and Collett, T. and More, A. and McCarthy, C.},
  year = {2017},
  month = oct,
  volume = {471},
  pages = {167--181},
  issn = {0035-8711},
  doi = {10.1093/mnras/stx1492},
  url = {https://academic.oup.com/mnras/article/471/1/167/3868797/Finding-strong-lenses-in-CFHTLS-using},
  urldate = {2017-10-16},
  abstract = {We train and apply convolutional neural networks, a machine learning technique developed to learn from and classify image data, to Canada\textendash{}France\textendash{}Hawaii Telescope Legacy Survey (CFHTLS) imaging for the identification of potential strong lensing systems. An ensemble of four convolutional neural networks was trained on images of simulated galaxy\textendash{}galaxy lenses. The training sets consisted of a total of 62~406 simulated lenses and 64~673 non-lens negative examples generated with two different methodologies. An ensemble of trained networks was applied to all of the 171~deg2 of the CFHTLS wide field image data, identifying 18~861 candidates including 63 known and 139 other potential lens candidates. A second search of 1.4 million early-type galaxies selected from the survey catalogue as potential deflectors, identified 2465 candidates including 117 previously known lens candidates, 29 confirmed lenses/high-quality lens candidates, 266 novel probable or potential lenses and 2097 candidates we classify as false positives. For the catalogue-based search we estimate a completeness of 21\textendash{}28~per~cent with respect to detectable lenses and a purity of 15~per~cent, with a false-positive rate of 1 in 671 images tested. We predict a human astronomer reviewing candidates produced by the system would identify 20 probable lenses and 100 possible lenses per hour in a sample selected by the robot. Convolutional neural networks are therefore a promising tool for use in the search for lenses in current and forthcoming surveys such as the Dark Energy Survey and the Large Synoptic Survey Telescope.},
  copyright = {All rights reserved},
  file = {/home/coljac/Zotero/storage/FH6DC8TA/Jacobs et al. - 2017 - Finding strong lenses in CFHTLS using convolutiona.pdf;/home/coljac/Zotero/storage/233V9K5P/stx1492.html},
  journal = {Mon Not R Astron Soc},
  number = {1}
}

@inproceedings{selvarajuGradCAMVisualExplanations2017,
  title = {Grad-{{CAM}}: {{Visual Explanations From Deep Networks}} via {{Gradient}}-{{Based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2017},
  pages = {618--626},
  url = {http://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html},
  urldate = {2019-07-16},
  file = {/home/coljac/Zotero/storage/F7W43JBJ/Selvaraju et al. - 2017 - Grad-CAM Visual Explanations From Deep Networks v.pdf;/home/coljac/Zotero/storage/EBU4PV83/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html}
}

@article{simonyanDeepConvolutionalNetworks2013,
  title = {Deep inside Convolutional Networks: {{Visualising}} Image Classification Models and Saliency Maps},
  shorttitle = {Deep inside Convolutional Networks},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  year = {2013},
  url = {https://arxiv.org/abs/1312.6034},
  urldate = {2017-04-19},
  file = {/home/coljac/Zotero/storage/8G575WF5/Simonyan et al_2013_Deep inside convolutional networks.pdf;/home/coljac/Zotero/storage/88P8VFPR/1312.html},
  journal = {arXiv preprint arXiv:1312.6034}
}

@article{smilkovSmoothGradRemovingNoise2017,
  title = {{{SmoothGrad}}: Removing Noise by Adding Noise},
  shorttitle = {{{SmoothGrad}}},
  author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  year = {2017},
  month = jun,
  url = {http://arxiv.org/abs/1706.03825},
  urldate = {2019-10-17},
  abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
  archivePrefix = {arXiv},
  eprint = {1706.03825},
  eprinttype = {arxiv},
  file = {/home/coljac/Zotero/storage/IFBTUGJH/IFBTUGJH.pdf;/home/coljac/Zotero/storage/GYQFIQ7T/1706.html},
  journal = {arXiv:1706.03825 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@incollection{zeilerVisualizingUnderstandingConvolutional2014,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2014},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  volume = {8689},
  pages = {818--833},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  url = {http://link.springer.com/10.1007/978-3-319-10590-1_53},
  urldate = {2016-02-19},
  file = {/home/coljac/Zotero/storage/2S76DX3R/Zeiler_Fergus_2014_Visualizing and Understanding Convolutional Networks.pdf},
  isbn = {978-3-319-10589-5 978-3-319-10590-1},
  keywords = {convnets,machine learning,toread}
}


